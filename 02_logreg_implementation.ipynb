{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce17a65c",
   "metadata": {},
   "source": [
    "# Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "0a0facdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38991eb",
   "metadata": {},
   "source": [
    "# Importando o dataset, renomeando coluna, e split em treino e teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "179cd854",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Covid Data.csv')\n",
    "data = data.rename(columns = {'CLASIFFICATION_FINAL': 'CLASSIFICATION_FINAL'})\n",
    "data1 = data.copy()\n",
    "data1.loc[data1['DATE_DIED'] == '9999-99-99', 'DIED'] = 2\n",
    "data1.loc[data1['DATE_DIED'] != '9999-99-99', 'DIED'] = 1\n",
    "data1.drop(columns = ['DATE_DIED'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "1c3de399",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = 'DIED'\n",
    "features = list(set(data1.columns).difference({label}))\n",
    "X = data1[features]\n",
    "y = data1[label]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28000615",
   "metadata": {},
   "source": [
    "# Pré-Processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "3eafde64",
   "metadata": {},
   "outputs": [],
   "source": [
    "boolean_features = ['PNEUMONIA', 'PREGNANT', 'DIABETES', 'COPD', 'ASTHMA', 'INMSUPR', 'HIPERTENSION',\n",
    "                   'CARDIOVASCULAR', 'RENAL_CHRONIC', 'OTHER_DISEASE', 'OBESITY', 'TOBACCO',\n",
    "                   'INTUBED', 'ICU']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "72a4fdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_boolean_columns(data, boolean_features):\n",
    "    '''Given a Pandas DataFrame and a list of string feature names, this function creates\n",
    "    new boolean columns for each feature in the list. The new columns are 1 if the value\n",
    "    of the feature is less than 3, and 0 otherwise.'''\n",
    "    new_data = data.copy()\n",
    "    for feature in boolean_features:\n",
    "        new_data.loc[new_data[feature] < 3, f'is_{feature}_defined'] = 1\n",
    "        new_data.loc[new_data[feature] >= 3, f'is_{feature}_defined'] = 2\n",
    "    return new_data\n",
    "\n",
    "def correct_pregnant_for_men(data):\n",
    "    '''Given a Pandas DataFrame, this function sets the value of the 'PREGNANT' feature to 0\n",
    "    for all rows where the value of the 'SEX' feature is 2 (corresponding to men).'''\n",
    "    new_data = data.copy()\n",
    "    new_data.loc[new_data['SEX'] == 2, 'PREGNANT'] = 0\n",
    "    return new_data\n",
    "\n",
    "def mode_imputation(data, pre_imputation_train_data, boolean_features):\n",
    "    '''Given a Pandas DataFrame, a Pandas DataFrame with the original training data used to\n",
    "    create the model, and a list of string feature names, this function imputes the mode\n",
    "    value for each feature in the list for all rows where the value of the feature is 3\n",
    "    or above (corresponding to missing values).'''\n",
    "    new_data = data.copy()\n",
    "    for feature in boolean_features:\n",
    "        most_common = pre_imputation_train_data[feature].mode()[0]\n",
    "        new_data.loc[new_data[feature] >= 3, feature] = most_common\n",
    "    return new_data\n",
    "\n",
    "def intubed_and_icu_imputation(data):\n",
    "    '''Given a Pandas DataFrame, this function sets the value of the 'INTUBED' and 'ICU'\n",
    "    features to 3 (corresponding to missing values) for all rows where the value is 3 or above.'''\n",
    "    new_data = data.copy()\n",
    "    more_nan_features = ['INTUBED', 'ICU']\n",
    "    for feature in more_nan_features:\n",
    "        new_data.loc[new_data[feature] >= 3, feature] = 3\n",
    "    return new_data\n",
    "\n",
    "def covid_degree(data):\n",
    "    '''takes in a pandas DataFrame and returns a copy of the DataFrame with an added column called 'covid_degree'.\n",
    "    The 'covid_degree' column is based on the 'CLASSIFICATION_FINAL' column in the input DataFrame. \n",
    "    If the value in the 'CLASSIFICATION_FINAL' column is greater than or equal to 4, the corresponding value in the 'covid_degree' column is 0. \n",
    "    If the value in the 'CLASSIFICATION_FINAL' column is less than 4, the corresponding value in the 'covid_degree' column \n",
    "    is the same as the value in the 'CLASSIFICATION_FINAL' column. \n",
    "    The 'CLASSIFICATION_FINAL' column is then dropped from the DataFrame.'''\n",
    "    new_data = data.copy()\n",
    "    new_data.loc[new_data['CLASSIFICATION_FINAL'] >= 4, 'covid_degree'] = 0\n",
    "    new_data.loc[new_data['CLASSIFICATION_FINAL'] < 4, 'covid_degree'] = new_data['CLASSIFICATION_FINAL']\n",
    "    new_data.drop('CLASSIFICATION_FINAL', axis = 1, inplace = True)\n",
    "    return new_data\n",
    "\n",
    "def scale(feature, unscaled_train_feature):\n",
    "    '''Scales a feature so that its values lie between 0 and 1.'''\n",
    "    minimum = min(unscaled_train_feature)\n",
    "    maximum = max(unscaled_train_feature)\n",
    "    return (feature - minimum)/(maximum - minimum)\n",
    "\n",
    "def binary_change(data):\n",
    "    '''This function takes in a pandas DataFrame or a list or numpy array \n",
    "    and returns a copy of the DataFrame or array with all 2's replaced with 0's.'''\n",
    "    new_data = data.copy()\n",
    "    new_data.loc[new_data == 2] = 0\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "267ecd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing_pipeline(X_train, X_test, y_train, y_test, boolean_features):\n",
    "    '''Applies each pre-processing function in the training and test datas.'''\n",
    "    X_train = create_boolean_columns(X_train, boolean_features)\n",
    "    X_test = create_boolean_columns(X_test, boolean_features)\n",
    "    \n",
    "    X_train = correct_pregnant_for_men(X_train)\n",
    "    X_test = correct_pregnant_for_men(X_test)\n",
    "    \n",
    "    pre_imputation_X_train = X_train.copy()\n",
    "    X_train = mode_imputation(X_train, pre_imputation_X_train, boolean_features)\n",
    "    X_test = mode_imputation(X_test, pre_imputation_X_train, boolean_features)\n",
    "    \n",
    "    X_train = intubed_and_icu_imputation(X_train)\n",
    "    X_test = intubed_and_icu_imputation(X_test)\n",
    "    \n",
    "    X_train = covid_degree(X_train)\n",
    "    X_test = covid_degree(X_test)\n",
    "    \n",
    "    features = X_train.columns\n",
    "    unscaled_X_train = X_train.copy()\n",
    "    \n",
    "    for feature in features:\n",
    "        X_train[feature] = scale(X_train[feature], unscaled_X_train[feature])\n",
    "        X_test[feature] = scale(X_test[feature], unscaled_X_train[feature])\n",
    "        \n",
    "    y_train = binary_change(y_train)\n",
    "    y_test = binary_change(y_test)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e08680b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = pre_processing_pipeline(X_train, X_test, y_train, y_test, boolean_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "1a6f77b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "814d3f88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ASTHMA</th>\n",
       "      <th>OBESITY</th>\n",
       "      <th>AGE</th>\n",
       "      <th>HIPERTENSION</th>\n",
       "      <th>PREGNANT</th>\n",
       "      <th>OTHER_DISEASE</th>\n",
       "      <th>SEX</th>\n",
       "      <th>RENAL_CHRONIC</th>\n",
       "      <th>INTUBED</th>\n",
       "      <th>COPD</th>\n",
       "      <th>...</th>\n",
       "      <th>is_INMSUPR_defined</th>\n",
       "      <th>is_HIPERTENSION_defined</th>\n",
       "      <th>is_CARDIOVASCULAR_defined</th>\n",
       "      <th>is_RENAL_CHRONIC_defined</th>\n",
       "      <th>is_OTHER_DISEASE_defined</th>\n",
       "      <th>is_OBESITY_defined</th>\n",
       "      <th>is_TOBACCO_defined</th>\n",
       "      <th>is_INTUBED_defined</th>\n",
       "      <th>is_ICU_defined</th>\n",
       "      <th>covid_degree</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>592908</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.239669</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184386</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.223140</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1021782</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.206612</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59606</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.214876</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93792</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.380165</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ASTHMA  OBESITY       AGE  HIPERTENSION  PREGNANT  OTHER_DISEASE  \\\n",
       "592908      1.0      0.0  0.239669           1.0       0.0            1.0   \n",
       "184386      1.0      1.0  0.223140           0.0       0.0            1.0   \n",
       "1021782     1.0      1.0  0.206612           1.0       0.0            1.0   \n",
       "59606       1.0      0.0  0.214876           1.0       1.0            1.0   \n",
       "93792       1.0      1.0  0.380165           1.0       1.0            1.0   \n",
       "\n",
       "         SEX  RENAL_CHRONIC  INTUBED  COPD  ...  is_INMSUPR_defined  \\\n",
       "592908   1.0            1.0      1.0   1.0  ...                 0.0   \n",
       "184386   1.0            1.0      1.0   1.0  ...                 0.0   \n",
       "1021782  1.0            1.0      1.0   1.0  ...                 0.0   \n",
       "59606    0.0            1.0      1.0   1.0  ...                 0.0   \n",
       "93792    0.0            1.0      0.5   1.0  ...                 0.0   \n",
       "\n",
       "         is_HIPERTENSION_defined  is_CARDIOVASCULAR_defined  \\\n",
       "592908                       0.0                        0.0   \n",
       "184386                       0.0                        0.0   \n",
       "1021782                      0.0                        0.0   \n",
       "59606                        0.0                        0.0   \n",
       "93792                        0.0                        0.0   \n",
       "\n",
       "         is_RENAL_CHRONIC_defined  is_OTHER_DISEASE_defined  \\\n",
       "592908                        0.0                       0.0   \n",
       "184386                        0.0                       0.0   \n",
       "1021782                       0.0                       0.0   \n",
       "59606                         0.0                       0.0   \n",
       "93792                         0.0                       0.0   \n",
       "\n",
       "         is_OBESITY_defined  is_TOBACCO_defined  is_INTUBED_defined  \\\n",
       "592908                  0.0                 0.0                 1.0   \n",
       "184386                  0.0                 0.0                 1.0   \n",
       "1021782                 0.0                 0.0                 1.0   \n",
       "59606                   0.0                 0.0                 1.0   \n",
       "93792                   0.0                 0.0                 0.0   \n",
       "\n",
       "         is_ICU_defined  covid_degree  \n",
       "592908              1.0           1.0  \n",
       "184386              1.0           0.0  \n",
       "1021782             1.0           0.0  \n",
       "59606               1.0           1.0  \n",
       "93792               0.0           1.0  \n",
       "\n",
       "[5 rows x 34 columns]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "27378be6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "592908     0.0\n",
       "184386     0.0\n",
       "1021782    0.0\n",
       "59606      0.0\n",
       "93792      0.0\n",
       "Name: DIED, dtype: float64"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e59804",
   "metadata": {},
   "source": [
    "# Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbe9828",
   "metadata": {},
   "source": [
    "A função de custo utilizada no modelo de regressão logística é a log loss:\n",
    "\n",
    "$$Log\\;Loss = -\\frac{1}{n}\\sum_{i = 1}^{n} y\\;log(\\hat{y}) + (1-y)\\;log(1-\\hat{y})$$\n",
    "\n",
    "sendo y a label do dataset de treino (0 ou 1) e $\\hat{y}$ o valor da label previsto pelo modelo (algo entre 0 e 1) para dados valores de features. O valor de $\\hat{y}$ é obtido pelo uso da função de ativação sigmoid em um modelo linear. Assim, definimos:\n",
    "\n",
    "$$z = b + \\sum_{i = 1}^{m} w_{i} x_{i}$$\n",
    "\n",
    "$$\\hat{y} = sigmoid(z) = sig(z) = \\frac{1}{1 + e^{-z}}$$\n",
    "\n",
    "em que $w_{i}$ é o peso da feature $x_{i}$ no modelo.\n",
    "\n",
    "Nesse modelo, também aplicaremos a regularização $L_{1}$, responsável por levar os pesos de features pouco importantes (com pesos muito pequenos) a exatamente 0, realizando, assim, uma seleção de features que gera um modelo mais esparso. A função de regularização $L_{1}$ é dada por:\n",
    "\n",
    "$L_{1}\\;regularization\\;term = \\sum_{i = 1}^{m} |w_{i}|$\n",
    "\n",
    "Assim, a função que vamos querer minimizar é:\n",
    "\n",
    "$$f(\\theta) = Log\\;Loss + L_{1}\\;regularization\\;term = -\\frac{1}{n}\\sum_{i = 1}^{n} y_{i}\\;log(\\hat{y}_{i}) + (1-y_{i})\\;log(1-\\hat{y}_{i}) + \\lambda\\sum_{i = 1}^{m} |w_{i}|$$\n",
    "\n",
    "onde $\\lambda$ é a constante de regularização, $\\theta = (w_{1}, w_{2}, ..., w_{n}, b)$, sendo $n$ o número de linhas de dados que temos no dataset de treino e $m$ o número de features.\n",
    "\n",
    "Sabemos que: $\\frac{d\\hat{y}_{i}}{dz_{i}} = sig(z_{i}) \\cdot (1-sig(z_{i}))$ (verifique!)\n",
    "\n",
    "Assim, podemos calcular as derivadas parciais de $\\hat{y}_{i}$: \n",
    "\n",
    "$$\\frac{\\partial{\\hat{y}_{i}}}{\\partial{w_{k}}} = \\frac{d\\hat{y}_{i}}{dz_{i}} \\cdot \\frac{\\partial{z_{i}}}{\\partial{w_{k}}} = sig(z_{i})\\cdot(1-sig(z_{i}))\\cdot x_{k}$$\n",
    "\n",
    "para toda feature $x_{k}$, e:\n",
    "\n",
    "$$\\frac{\\partial{\\hat{y}_{i}}}{\\partial{b}} = \\frac{d\\hat{y}_{i}}{dz_{i}} \\cdot \\frac{\\partial{z_{i}}}{\\partial{b}} = sig(z_{i})\\cdot(1-sig(z_{i}))$$\n",
    "\n",
    "Calculando agora as derivadas parciais de $f(\\theta)$:\n",
    "\n",
    "$$\\frac{\\partial{f}}{\\partial{w_{k}}} = -\\frac{1}{n}\\sum_{i = 1}^{n}\\left[y_{i}\\cdot \\frac{sig(z_{i})\\cdot(1-sig(z_{i}))\\cdot (x_{k})_{i}}{\\hat{y}_{i}} - (1-y_{i})\\cdot\\frac{sig(z_{i})\\cdot(1-sig(z_{i}))\\cdot (x_{k})_{i}}{(1-\\hat{y}_{i})}\\right] + \\lambda \\frac{|w_{k}|}{w_{k}}$$\n",
    "\n",
    "$$\\frac{\\partial{f}}{\\partial{b}} = -\\frac{1}{n}\\sum_{i = 1}^{n}\\left[y_{i}\\cdot \\frac{sig(z_{i})\\cdot(1-sig(z_{i}))}{\\hat{y}_{i}} - (1-y_{i})\\cdot\\frac{sig(z_{i})\\cdot(1-sig(z_{i}))}{(1-\\hat{y}_{i})}\\right]$$\n",
    "\n",
    "Assim, sendo $\\alpha$ a taxa de aprendizado do modelo, os novos valores dos pesos e do viés serão:\n",
    "\n",
    "$$w_{k}' = w_{k} - \\alpha \\frac{\\partial{f}}{\\partial{w_{k}}}$$\n",
    "\n",
    "$$b' = b - \\alpha \\frac{\\partial{f}}{\\partial{b}}$$\n",
    "\n",
    "Desenvolvendo, temos, por fim:\n",
    "\n",
    "$$w_{k}' = w_{k} + \\frac{\\alpha}{n}\\sum_{i = 1}^{n}(x_{k})_{i}\\cdot\\left[y_{i}\\cdot {(1-\\hat{y}_{i})} - (1-y_{i})\\cdot \\hat{y}_{i}\\right] - \\alpha \\lambda \\frac{|w_{k}|}{w_{k}}$$\n",
    "\n",
    "$$b' = b + \\frac{\\alpha}{n}\\sum_{i = 1}^{n}\\left[y_{i}\\cdot {(1-\\hat{y}_{i})} - (1-y_{i})\\cdot \\hat{y}_{i}\\right] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b38776",
   "metadata": {},
   "source": [
    "# Implementação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "8544fe5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logistic_Regression():\n",
    "    def __init__(self, X_train, y_train, ws: list,\n",
    "                 b = 0, alpha = 0.1, lambda_reg = 0.1, random_state = 0):\n",
    "        '''Constructor for the Logistic_Regression class. It initializes the following attributes:\n",
    "        - features: a pandas DataFrame containing the training features\n",
    "        - label: a pandas Series containing the training labels\n",
    "        - ws: a numpy array containing the weights\n",
    "        - b: a float representing the bias term\n",
    "        - alpha: a float representing the learning rate\n",
    "        - lambda_reg: a float representing the regularization strength\n",
    "        - rand: a random number generator with a specified seed (random_state)'''\n",
    "        self.features = X_train\n",
    "        self.label = y_train\n",
    "        ws = [float(w) for w in ws]\n",
    "        self.ws = np.array(ws) # weights\n",
    "        self.b = b\n",
    "        self.alpha = alpha\n",
    "        self.lambda_reg = lambda_reg\n",
    "        self.rand = np.random.RandomState(random_state)\n",
    "        \n",
    "    def print_parameters(self):\n",
    "        '''Prints the weights and bias term.'''\n",
    "        for i in range(1, len(self.ws) + 1):\n",
    "            print(f'w{i} = {self.ws[i - 1]}')\n",
    "        print (f'b = {self.b}')\n",
    "        \n",
    "    def get_parameters(self):\n",
    "        '''Returns a dictionary containing the weights and bias term. \n",
    "        The keys are 'w1', 'w2', etc. for the weights, and 'b' for the bias term.'''\n",
    "        i_vals = list(range(1, len(self.ws) + 1))\n",
    "        parameters = {f'w{i}': self.ws[i - 1] for i in i_vals}\n",
    "        parameters['b'] = self.b\n",
    "        return parameters\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid(z):\n",
    "        '''Takes in a float or a numpy array and returns the sigmoid of the input.'''\n",
    "        return 1/(1 + np.exp(-z))\n",
    "    \n",
    "    def predict(self, X):\n",
    "        '''Takes in a Pandas DataFrame X containing feature values \n",
    "        and returns a NumPy array of predicted values of the label using \n",
    "        the current values of ws and b.'''\n",
    "        n = len(X)\n",
    "        X_copy = X.copy()\n",
    "        X_copy.reset_index(inplace = True, drop = True)\n",
    "        X_copy = X.mul(self.ws)\n",
    "        predictions = X_copy.sum(axis = 1) + self.b\n",
    "        predictions = Logistic_Regression.sigmoid(predictions)\n",
    "        return predictions\n",
    "    \n",
    "    def get_loss(self, X, y):\n",
    "        '''Takes in a pandas DataFrame containing the features (X) \n",
    "        and a pandas Series (y) containing the labels, \n",
    "        and returns the log loss for the given data.'''\n",
    "        predictions = self.predict(X)\n",
    "        fst_term = y * np.log(predictions)\n",
    "        sec_term = (1 - y) * np.log(1 - predictions)\n",
    "        loss = -np.mean(fst_term + sec_term)\n",
    "        return loss\n",
    "    \n",
    "    def get_l1_term(self):\n",
    "        '''Gets L1 regularization term for the model.'''\n",
    "        return np.sum(np.abs(self.ws))\n",
    "    \n",
    "    def get_structural_risk(self, X_test, y_test):\n",
    "        '''Gets structural risk for the model in X_test features and y_test label.'''\n",
    "        return self.get_loss(X_test, y_test) + self.lambda_reg * self.get_l1_term\n",
    "    \n",
    "    def __get_Xy_sample(self, begin_index, end_index):\n",
    "        '''Private helper function which from X and y starting in the begin_index \n",
    "        row and ending in end_index row.'''\n",
    "        X = self.features.iloc[begin_index:end_index]\n",
    "        y = self.label.iloc[begin_index:end_index]\n",
    "        return X, y\n",
    "    \n",
    "    def __get_partial_reg_terms(self):\n",
    "        get_partial_reg_term = np.vectorize(lambda w: 0 if w == 0 else np.abs(w)/w)\n",
    "        return get_partial_reg_term(self.ws)\n",
    "    \n",
    "    def __get_sample_partial_w(self, X, diff, batch_size):\n",
    "        '''Private helper function which gets the partial derivative of loss with \n",
    "        respect to weights for a sample (X).'''\n",
    "        partial_reg_terms = self.__get_partial_reg_terms()\n",
    "        partial_w = (-diff / batch_size) @ X + self.lambda_reg * partial_reg_terms\n",
    "        return partial_w\n",
    "    \n",
    "    def __get_sample_partial_b(self, diff, batch_size):\n",
    "        partial_b = -(1/batch_size) * np.sum(diff)\n",
    "        return partial_b\n",
    "    \n",
    "    def __batch_update_parameters(self, X, diff, batch_size, inexact_batch_size):\n",
    "        '''Private helper function which updates weights (ws) and bias (b) for a batch.'''\n",
    "        partial_w = self.__get_sample_partial_w(X, diff, inexact_batch_size)\n",
    "        partial_b = self.__get_sample_partial_b(diff, inexact_batch_size)\n",
    "        correction_constant = batch_size/inexact_batch_size\n",
    "        self.ws -= self.alpha * partial_w * correction_constant\n",
    "        self.b -= self.alpha * partial_b * correction_constant\n",
    "        \n",
    "    def __sgd_update_parameters(self, batch_size: int):\n",
    "        '''Private helper function that performs one step of stochastic gradient descent on the model's parameters (ws and b). \n",
    "        It takes in a single argument batch_size, which is the number of samples to use in the mini-batch for this step of gradient descent. \n",
    "        The function first selects mini-batches from the training data, and then performs an update to the model's parameters \n",
    "        using the gradient of the mean squared error loss with respect to the parameters for each mini-batch.\n",
    "        The update is performed using the learning rate alpha.'''\n",
    "        num_of_data_rows = len(self.label)\n",
    "        inexact_batch_size = num_of_data_rows % batch_size\n",
    "        num_of_exact_batches = int(num_of_data_rows/batch_size)\n",
    "        for exact_batch in range(1, num_of_exact_batches + 1):\n",
    "            begin_index = (exact_batch - 1) * batch_size\n",
    "            end_index = exact_batch * batch_size\n",
    "            X, y = self.__get_Xy_sample(begin_index, end_index)\n",
    "            predictions = self.predict(X)\n",
    "            fst_term = y * (1 - predictions)\n",
    "            sec_term = (1 - y) * predictions\n",
    "            diff = fst_term - sec_term\n",
    "            self.__batch_update_parameters(X, diff, batch_size, batch_size)\n",
    "        if inexact_batch_size != 0:\n",
    "            begin_index = (num_of_exact_batches) * batch_size + 1\n",
    "            end_index = num_of_data_rows + 1\n",
    "            X, y = self.__get_Xy_sample(begin_index, end_index)\n",
    "            predictions = self.predict(X)\n",
    "            fst_term = y * (1 - predictions)\n",
    "            sec_term = (1 - y) * predictions\n",
    "            diff = fst_term - sec_term\n",
    "            self.__batch_update_parameters(X, diff, batch_size, inexact_batch_size)\n",
    "        \n",
    "    def sgd(self, iterations: int, batch_size: float, print_loss = False): # stochastic gradient descent\n",
    "        '''Performs stochastic gradient descent for a specified number of iterations.'''\n",
    "        for i in range(0, iterations):\n",
    "            self.__sgd_update_parameters(batch_size)\n",
    "            if print_loss:\n",
    "                print(f'loss = {self.get_loss(self.features, self.label)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "de36a6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = list(np.zeros(len(features)))\n",
    "model = Logistic_Regression(X_train = X_train, y_train = y_train, ws = ws, lambda_reg = 0.01, alpha = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "56086385",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 0.25277087235830625\n",
      "loss = 0.22969711330010847\n",
      "loss = 0.21573104893821424\n",
      "loss = 0.2082657248450225\n",
      "loss = 0.20437863836128545\n",
      "loss = 0.20156243154154205\n",
      "loss = 0.19899296794019258\n",
      "loss = 0.19654858471517273\n",
      "loss = 0.1942892097179991\n",
      "loss = 0.19224464690474152\n",
      "loss = 0.19021320782163195\n",
      "loss = 0.18833480205529682\n",
      "loss = 0.18657770711848887\n",
      "loss = 0.1850015245541661\n",
      "loss = 0.18348433172192788\n",
      "loss = 0.18205280680554464\n",
      "loss = 0.180638690032133\n",
      "loss = 0.1793218842584825\n",
      "loss = 0.17812100312238435\n",
      "loss = 0.17695845789264486\n",
      "loss = 0.1758730408237387\n",
      "loss = 0.1749336055245433\n",
      "loss = 0.17390631572689388\n",
      "loss = 0.1730125037319838\n",
      "loss = 0.1721788218870454\n",
      "loss = 0.1714142227060946\n",
      "loss = 0.17066542371856608\n",
      "loss = 0.1699283174844078\n",
      "loss = 0.16928594583391746\n",
      "loss = 0.16863227307370607\n",
      "loss = 0.16809109725797453\n",
      "loss = 0.1674717553697401\n",
      "loss = 0.16695080619905617\n",
      "loss = 0.16645513279902363\n",
      "loss = 0.16602351871225504\n",
      "loss = 0.1653950664826844\n",
      "loss = 0.16509998020563837\n",
      "loss = 0.1647373661252006\n",
      "loss = 0.16435050804385057\n",
      "loss = 0.16394777431426646\n",
      "loss = 0.16370056980755593\n",
      "loss = 0.16339213233859354\n",
      "loss = 0.16300950582731255\n",
      "loss = 0.1627316863125865\n",
      "loss = 0.16253313827073557\n",
      "loss = 0.16222971956732146\n",
      "loss = 0.16199914955556188\n",
      "loss = 0.16178890164721985\n",
      "loss = 0.16155330120204242\n",
      "loss = 0.16132201723750178\n",
      "loss = 0.16112954364955104\n",
      "loss = 0.16099861418355338\n",
      "loss = 0.16072345665695445\n",
      "loss = 0.16050831689748893\n",
      "loss = 0.16036221379559426\n",
      "loss = 0.1602489376111289\n",
      "loss = 0.1600869013575198\n",
      "loss = 0.1598879013824165\n",
      "loss = 0.15983179201084638\n",
      "loss = 0.1596954687200848\n",
      "loss = 0.15957101536934776\n",
      "loss = 0.15941623258323037\n",
      "loss = 0.15943756340170148\n",
      "loss = 0.15925764152457458\n",
      "loss = 0.15921349377383823\n",
      "loss = 0.15907626007182576\n",
      "loss = 0.15890241454268644\n",
      "loss = 0.1588306173958625\n",
      "loss = 0.15886376541413147\n",
      "loss = 0.15880408620930583\n",
      "loss = 0.15863332899122268\n",
      "loss = 0.15865127964587286\n",
      "loss = 0.15861209435565585\n",
      "loss = 0.15848894918667789\n",
      "loss = 0.15834718092486855\n",
      "loss = 0.15845704113891979\n",
      "loss = 0.15834594776480837\n",
      "loss = 0.15827285494545773\n",
      "loss = 0.15820755851017382\n",
      "loss = 0.1582964884843503\n",
      "loss = 0.15821615788167098\n",
      "loss = 0.15803773451177394\n",
      "loss = 0.1581444810854476\n",
      "loss = 0.15806157600426887\n",
      "loss = 0.15794354100634206\n",
      "loss = 0.15801427775243054\n",
      "loss = 0.1578778775778544\n",
      "loss = 0.15794533156681126\n",
      "loss = 0.15789709192841625\n",
      "loss = 0.15786249125587976\n",
      "loss = 0.1578080592313015\n",
      "loss = 0.15774105648577916\n",
      "loss = 0.1578862469218301\n",
      "loss = 0.15764750077658932\n",
      "loss = 0.1577291667289772\n",
      "loss = 0.15768340595668226\n",
      "loss = 0.1576232265880913\n",
      "loss = 0.157698719047676\n",
      "loss = 0.1576122767786904\n",
      "loss = 0.15757877362485068\n"
     ]
    }
   ],
   "source": [
    "model.sgd(iterations = 100, batch_size = 1000000, print_loss = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "39950b5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "875680     0.020914\n",
       "1046906    0.020975\n",
       "646861     0.022825\n",
       "704385     0.021049\n",
       "798051     0.020930\n",
       "             ...   \n",
       "986581     0.021062\n",
       "916423     0.020920\n",
       "73306      0.023098\n",
       "565582     0.023277\n",
       "754044     0.020920\n",
       "Length: 209715, dtype: float64"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "0b040627",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contagem dos valores da predição:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    177412\n",
       "1     32303\n",
       "dtype: int64"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threshold = 0.19\n",
    "print('Contagem dos valores da predição:')\n",
    "(pd.DataFrame(model.predict(X_test)) > threshold).astype(int).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9444a83",
   "metadata": {},
   "source": [
    "#### Conclusão\n",
    "\n",
    "Apesar do número reduzido de iterações e da velocidade menor em relação às bibliotecas de Machine Learning já implementadas, conseguimos um resultado válido. Isso mostra que nosso modelo funciona, de fato. \n",
    "\n",
    "Não implementaremos as métricas do modelo de regressão logística porque são relativamente simples e o objetivo era conseguir implementar tal modelo do zero com o grande desafio de tratar milhões de dado simultaneamente na função de predição com uma velocidade que permitisse o treinamento do modelo em tempo hábil. A evaluação das métricas pode ser feita utilizando a biblioteca sklearn, conforme foi feito no notebook 01, que trata de feature engineering passo-a-passo e de uma aplicação mais prática do modelo de regressão logística."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
