{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce17a65c",
   "metadata": {},
   "source": [
    "# Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a0facdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38991eb",
   "metadata": {},
   "source": [
    "# Importando o dataset, renomeando coluna e split em treino e teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "179cd854",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Covid Data.csv')\n",
    "data = data.rename(columns = {'CLASIFFICATION_FINAL': 'CLASSIFICATION_FINAL'})\n",
    "data1 = data.copy()\n",
    "data1.loc[data1['DATE_DIED'] == '9999-99-99', 'DIED'] = 2\n",
    "data1.loc[data1['DATE_DIED'] != '9999-99-99', 'DIED'] = 1\n",
    "data1.drop(columns = ['DATE_DIED'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c3de399",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = 'DIED'\n",
    "features = list(set(data1.columns).difference({label}))\n",
    "X = data1[features]\n",
    "y = data1[label]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28000615",
   "metadata": {},
   "source": [
    "# Pré-Processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3eafde64",
   "metadata": {},
   "outputs": [],
   "source": [
    "boolean_features = ['PNEUMONIA', 'PREGNANT', 'DIABETES', 'COPD', 'ASTHMA', 'INMSUPR', 'HIPERTENSION',\n",
    "                   'CARDIOVASCULAR', 'RENAL_CHRONIC', 'OTHER_DISEASE', 'OBESITY', 'TOBACCO',\n",
    "                   'INTUBED', 'ICU']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "72a4fdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_boolean_columns(data, boolean_features):\n",
    "    new_data = data.copy()\n",
    "    for feature in boolean_features:\n",
    "        new_data.loc[new_data[feature] < 3, f'is_{feature}_defined'] = 1\n",
    "        new_data.loc[new_data[feature] >= 3, f'is_{feature}_defined'] = 2\n",
    "    return new_data\n",
    "\n",
    "def correct_pregnant_for_men(data):\n",
    "    new_data = data.copy()\n",
    "    new_data.loc[new_data['SEX'] == 2, 'PREGNANT'] = 0\n",
    "    return new_data\n",
    "\n",
    "def mode_imputing(data, boolean_features):\n",
    "    new_data = data.copy()\n",
    "    for feature in boolean_features:\n",
    "        most_common = new_data[feature].mode()[0]\n",
    "        new_data.loc[new_data[feature] >= 3, feature] = most_common\n",
    "    return new_data\n",
    "\n",
    "def intubed_and_icu_imputing(data):\n",
    "    new_data = data.copy()\n",
    "    more_nan_features = ['INTUBED', 'ICU']\n",
    "    for feature in more_nan_features:\n",
    "        new_data.loc[new_data[feature] >= 3, feature] = 3\n",
    "    return new_data\n",
    "\n",
    "def covid_degree(data):\n",
    "    new_data = data.copy()\n",
    "    new_data.loc[new_data['CLASSIFICATION_FINAL'] >= 4, 'covid_degree'] = 0\n",
    "    new_data.loc[new_data['CLASSIFICATION_FINAL'] < 4, 'covid_degree'] = new_data['CLASSIFICATION_FINAL']\n",
    "    new_data.drop('CLASSIFICATION_FINAL', axis = 1, inplace = True)\n",
    "    return new_data\n",
    "\n",
    "def scale(feature):\n",
    "    minimum = min(feature)\n",
    "    maximum = max(feature)\n",
    "    return (feature - minimum)/(maximum - minimum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "267ecd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing_pipeline(X_train, X_test, boolean_features):\n",
    "    X_train = create_boolean_columns(X_train, boolean_features)\n",
    "    X_test = create_boolean_columns(X_test, boolean_features)\n",
    "    \n",
    "    X_train = correct_pregnant_for_men(X_train)\n",
    "    X_test = correct_pregnant_for_men(X_test)\n",
    "    \n",
    "    X_train = mode_imputing(X_train, boolean_features)\n",
    "    X_test = mode_imputing(X_test, boolean_features)\n",
    "    \n",
    "    X_train = intubed_and_icu_imputing(X_train)\n",
    "    X_test = intubed_and_icu_imputing(X_test)\n",
    "    \n",
    "    X_train = covid_degree(X_train)\n",
    "    X_test = covid_degree(X_test)\n",
    "    \n",
    "    features = X_train.columns\n",
    "    \n",
    "    for feature in features:\n",
    "        X_train[feature] = scale(X_train[feature])\n",
    "        X_test[feature] = scale(X_test[feature])\n",
    "    \n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e08680b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = pre_processing_pipeline(X_train, X_test, boolean_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e59804",
   "metadata": {},
   "source": [
    "# Implementação do modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbe9828",
   "metadata": {},
   "source": [
    "A função de custo utilizada no modelo de regressão logística é a log loss:\n",
    "\n",
    "$$Log\\;Loss = -\\frac{1}{n}\\sum_{i = 1}^{n} y\\;log(\\hat{y}) + (1-y)\\;log(1-\\hat{y})$$\n",
    "\n",
    "sendo y a label do dataset de treino (0 ou 1) e $\\hat{y}$ o valor da label previsto pelo modelo (algo entre 0 e 1) para dados valores de features. O valor de $\\hat{y}$ é obtido pelo uso da função de ativação sigmoid em um modelo linear. Assim, definimos:\n",
    "\n",
    "$$z = b + \\sum_{i = 1}^{m} w_{i} x_{i}$$\n",
    "\n",
    "$$\\hat{y} = sigmoid(z) = sig(z) = \\frac{1}{1 + e^{-z}}$$\n",
    "\n",
    "em que $w_{i}$ é o peso da feature $x_{i}$ no modelo.\n",
    "\n",
    "Nesse modelo, também aplicaremos a regularização $L_{1}$, responsável por levar os pesos de features pouco importantes (com pesos muito pequenos) a exatamente 0, realizando, assim, uma seleção de features que gera um modelo mais esparso. A função de regularização $L_{1}$ é dada por:\n",
    "\n",
    "$L_{1}\\;regularization\\;term = \\sum_{i = 1}^{m} |w_{i}|$\n",
    "\n",
    "Assim, a função que vamos querer minimizar é:\n",
    "\n",
    "$$f(\\theta) = Log\\;Loss + L_{1}\\;regularization\\;term = -\\frac{1}{n}\\sum_{i = 1}^{n} y_{i}\\;log(\\hat{y}_{i}) + (1-y_{i})\\;log(1-\\hat{y}_{i}) + \\lambda\\sum_{i = 1}^{m} |w_{i}|$$\n",
    "\n",
    "onde $\\lambda$ é a constante de regularização, $\\theta = (w_{1}, w_{2}, ..., w_{n}, b)$, sendo $n$ o número de linhas de dados que temos no dataset de treino e $m$ o número de features.\n",
    "\n",
    "Sabemos que: $\\frac{d\\hat{y}_{i}}{dz_{i}} = sig(z_{i}) \\cdot (1-sig(z_{i}))$ (verifique!)\n",
    "\n",
    "Assim, podemos calcular as derivadas parciais de $\\hat{y}_{i}$: \n",
    "\n",
    "$$\\frac{\\partial{\\hat{y}_{i}}}{\\partial{w_{k}}} = \\frac{d\\hat{y}_{i}}{dz_{i}} \\cdot \\frac{\\partial{z_{i}}}{\\partial{w_{k}}} = sig(z_{i})\\cdot(1-sig(z_{i}))\\cdot x_{k}$$\n",
    "\n",
    "para toda feature $x_{k}$, e:\n",
    "\n",
    "$$\\frac{\\partial{\\hat{y}_{i}}}{\\partial{b}} = \\frac{d\\hat{y}_{i}}{dz_{i}} \\cdot \\frac{\\partial{z_{i}}}{\\partial{b}} = sig(z_{i})\\cdot(1-sig(z_{i}))$$\n",
    "\n",
    "Calculando agora as derivadas parciais de $f(\\theta)$:\n",
    "\n",
    "$$\\frac{\\partial{f}}{\\partial{w_{k}}} = -\\frac{1}{n}\\sum_{i = 1}^{n}\\left[y_{i}\\cdot \\frac{sig(z_{i})\\cdot(1-sig(z_{i}))\\cdot (x_{k})_{i}}{\\hat{y}_{i}} - (1-y_{i})\\cdot\\frac{sig(z_{i})\\cdot(1-sig(z_{i}))\\cdot (x_{k})_{i}}{(1-\\hat{y}_{i})}\\right] + \\lambda \\frac{|w_{k}|}{w_{k}}$$\n",
    "\n",
    "$$\\frac{\\partial{f}}{\\partial{b}} = -\\frac{1}{n}\\sum_{i = 1}^{n}\\left[y_{i}\\cdot \\frac{sig(z_{i})\\cdot(1-sig(z_{i}))}{\\hat{y}_{i}} - (1-y_{i})\\cdot\\frac{sig(z_{i})\\cdot(1-sig(z_{i}))}{(1-\\hat{y}_{i})}\\right]$$\n",
    "\n",
    "Assim, sendo $\\alpha$ a taxa de aprendizado do modelo, os novos valores dos pesos e do viés serão:\n",
    "\n",
    "$$w_{k}' = w_{k} - \\alpha \\frac{\\partial{f}}{\\partial{w_{k}}}$$\n",
    "\n",
    "$$b' = b - \\alpha \\frac{\\partial{f}}{\\partial{b}}$$\n",
    "\n",
    "Desenvolvendo, temos, por fim:\n",
    "\n",
    "$$w_{k}' = w_{k} + \\frac{\\alpha}{n}\\sum_{i = 1}^{n}\\left[y_{i}\\cdot \\frac{sig(z_{i})\\cdot(1-sig(z_{i}))\\cdot (x_{k})_{i}}{\\hat{y}_{i}} - (1-y_{i})\\cdot\\frac{sig(z_{i})\\cdot(1-sig(z_{i}))\\cdot (x_{k})_{i}}{(1-\\hat{y}_{i})}\\right] + \\alpha \\lambda \\frac{|w_{k}|}{w_{k}}$$\n",
    "\n",
    "$$b' = b + \\frac{\\alpha}{n}\\sum_{i = 1}^{n}\\left[y_{i}\\cdot \\frac{sig(z_{i})\\cdot(1-sig(z_{i}))}{\\hat{y}_{i}} - (1-y_{i})\\cdot\\frac{sig(z_{i})\\cdot(1-sig(z_{i}))}{(1-\\hat{y}_{i})}\\right]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd92cee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
