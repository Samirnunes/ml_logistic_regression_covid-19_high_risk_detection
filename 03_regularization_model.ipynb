{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a0930eb",
   "metadata": {},
   "source": [
    "## Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6272481",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c55414",
   "metadata": {},
   "source": [
    "## Importando o dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bee9c893",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'adm_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12700/854170552.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'adm_data.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'Chance of Admit '\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'Chance of Admit'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'LOR '\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'LOR'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 586\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    587\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    481\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 482\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    483\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    484\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    810\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 811\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    812\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1038\u001b[0m             )\n\u001b[0;32m   1039\u001b[0m         \u001b[1;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1040\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1041\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1042\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;31m# open handles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m    220\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m         \"\"\"\n\u001b[1;32m--> 222\u001b[1;33m         self.handles = get_handle(\n\u001b[0m\u001b[0;32m    223\u001b[0m             \u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    700\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 702\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    703\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'adm_data.csv'"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('adm_data.csv')\n",
    "data.rename(columns = {'Chance of Admit ': 'Chance of Admit'}, inplace = True)\n",
    "data.rename(columns = {'LOR ': 'LOR'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7544dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['GRE Score', 'TOEFL Score', 'University Rating', 'SOP', 'LOR', 'CGPA', 'Research']\n",
    "label = 'Chance of Admit'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309b3e53",
   "metadata": {},
   "source": [
    "### Mudando a escala dos dados para [0, 1]\n",
    "\n",
    "Transformaremos [min, max] -> [0, 1] para cada coluna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b18a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(feature):\n",
    "    minimum = min(feature)\n",
    "    maximum = max(feature)\n",
    "    return (feature - minimum)/(maximum - minimum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f47ad20",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_scaled = data.copy()\n",
    "for feature in features:\n",
    "    data_scaled[feature] = scale(data[feature])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dee6bd6",
   "metadata": {},
   "source": [
    "## Modelo com Regularização para Complexidade\n",
    "\n",
    "Iremos modificar o modelo implementado anteriormente e adicionar o termo de complexidade à função de custo, definindo-o pela fórmula de regularização $L_{2}$ ($L_{2}$ Regularization formula). Tal termo é dado por:\n",
    "\n",
    "$L_{2}$ regularization term = $ \\sum_{i = 1}^{n} w_{i}^{2}$\n",
    "\n",
    "Definimos $\\lambda$ como o coeficiente de regularização, um hiperparâmetro do modelo que multiplica a função de complexidade. Quanto maior $\\lambda$, maior o efeito de regularização na complexidade, isto é, mais penalizados os pesos muito grandes nas features são. Dessa forma, a nova função que se quer minimizar no modelo é dada por:\n",
    "\n",
    "$f(\\theta)$ = Loss + $\\lambda$ Complexity = $\\frac{1}{n} \\sum_{i = 1}^{n} (\\hat{y}_{i} - y_{i})^{2} + \\lambda \\sum_{i = 1}^{n} w_{i}^{2}$\n",
    "\n",
    "cujas derivadas parciais são dadas por:\n",
    "\n",
    "$\\frac{\\partial f}{\\partial w_{k}} = \\frac{2}{n}\\sum_{i = 1}^{n} (\\hat{y}_{i} - y_{i}) \\cdot (x_{k})_{i} + 2\\lambda w_{k}$, para toda feature $x_{k}$ com peso $w_{k}$.\n",
    "\n",
    "$\\frac{\\partial f}{\\partial b} = \\frac{2}{n}\\sum_{i = 1}^{n} (\\hat{y}_{i} - y_{i})$\n",
    "\n",
    "Assim, obtemos os novos valores do peso e do viés após uma iteração:\n",
    "\n",
    "$w_{k}' = (1 - 2 \\alpha \\lambda)\\;w_{k} - \\frac{2 \\alpha}{n}\\sum_{i = 1}^{n} (\\hat{y}_{i} - y_{i}) \\cdot (x_{k})_{i}$, para toda feature $x_{k}$ com peso $w_{k}$.\n",
    "\n",
    "\n",
    "$b' = b - \\frac{2 \\alpha}{n}\\sum_{i = 1}^{n} (\\hat{y}_{i} - y_{i})$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a0082c",
   "metadata": {},
   "source": [
    "### Implementação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2565997",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Regularization_Linear_Regression_Model():\n",
    "    def __init__(self, train_data, features_name: list, label_name: str, ws: list, b: float, alpha: float, lambda_reg: float, random_state: int):\n",
    "        self.train_data = train_data\n",
    "        self.features = train_data[features_name]\n",
    "        self.label = train_data[label_name]\n",
    "        self.label_name = label_name # saving here because label as a series doesn't save it.\n",
    "        for i in range(0, len(ws)):\n",
    "            ws[i] = float(ws[i])\n",
    "        self.ws = np.array(ws) # weights\n",
    "        self.b = b\n",
    "        self.alpha = alpha\n",
    "        self.lambda_reg = lambda_reg\n",
    "        self.rand = np.random.RandomState(random_state)\n",
    "        \n",
    "    def print_parameters(self):\n",
    "        for i in range(1, len(self.ws) + 1):\n",
    "            print(f'w{i} = {self.ws[i - 1]}')\n",
    "        print (f'b = {self.b}')\n",
    "        \n",
    "    def get_parameters(self):\n",
    "        i_vals = list(range(1, len(self.ws) + 1))\n",
    "        parameters = {f'w{i}': self.ws[i - 1] for i in i_vals}\n",
    "        parameters['b'] = self.b\n",
    "        return parameters\n",
    "        \n",
    "    def get_single_prediction(self, xs: list):\n",
    "        '''Get the prediction for a list with all the features' values.'''\n",
    "        for i in range(0, len(xs)):\n",
    "            xs[i] = float(xs[i])\n",
    "        xs = np.array(xs)\n",
    "        pred = np.dot(self.ws, xs) + self.b\n",
    "        return pred\n",
    "    \n",
    "    def predict(self, data):\n",
    "        n = len(data)\n",
    "        features_name = self.features.columns\n",
    "        data_features = data[features_name]\n",
    "        predictions = np.zeros(n)\n",
    "        for i in range(0, n):\n",
    "            xs = list(data_features.iloc[i])\n",
    "            predictions[i] += self.get_single_prediction(xs)\n",
    "        return predictions\n",
    "    \n",
    "    def get_loss(self, data):\n",
    "        n = len(data)\n",
    "        data_label = data[self.label_name]\n",
    "        y = np.array(data_label)\n",
    "        predictions = self.predict(data)\n",
    "        diff = predictions - y\n",
    "        loss = (1/n)*np.dot(diff, diff)\n",
    "        return loss\n",
    "    \n",
    "    def get_complexity(self):\n",
    "        return np.dot(self.ws, self.ws)\n",
    "    \n",
    "    def get_structural_risk(self, data):\n",
    "        return self.get_loss(data) + self.lambda_reg * self.get_complexity()\n",
    "               \n",
    "    def sgd_update_parameters(self, batch_size: int):\n",
    "        n = len(self.label)\n",
    "        index_list = list(range(0, n))\n",
    "        random_indices = self.rand.choice(index_list, size = batch_size, replace = True) # bootstrap sample\n",
    "        xs_sample = list()\n",
    "        y_sample = np.array(self.label.iloc[random_indices])\n",
    "        preds_sample = np.zeros(batch_size)\n",
    "        for i in range(0, batch_size):\n",
    "            xs = list(self.features.iloc[random_indices[i]])\n",
    "            preds_sample[i] += self.get_single_prediction(xs)\n",
    "        for col in self.features:\n",
    "            xs_sample.append(np.array(self.features[col].iloc[random_indices])) # len(xs_sample) = len(self.ws)\n",
    "        diff_sample = preds_sample - y_sample\n",
    "        partial_w = np.zeros(len(self.ws))\n",
    "        for i in range(0, len(self.ws)):\n",
    "            partial_w[i] += (2/batch_size) * np.dot(diff_sample, xs_sample[i]) + 2*self.lambda_reg*self.ws[i]\n",
    "        partial_b = (2/batch_size) * np.sum(diff_sample)\n",
    "        self.ws -= self.alpha * partial_w\n",
    "        self.b -= self.alpha * partial_b\n",
    "        \n",
    "    def sgd(self, iterations: int, batch_size: float, print_loss: bool): # stochastic gradient descent\n",
    "        for i in range(0, iterations):\n",
    "            self.sgd_update_parameters(batch_size)\n",
    "            if print_loss:\n",
    "                print(f'loss = {self.get_loss(self.train_data)}')\n",
    "    \n",
    "    @staticmethod\n",
    "    def shuffle_data(data, random_state):\n",
    "        rand = np.random.RandomState(random_state)\n",
    "        return data.reindex(rand.permutation(data.index))\n",
    "    \n",
    "    @staticmethod\n",
    "    def train_val_test_split(data, test_split: float, val_split: float):\n",
    "        '''Get train, validation and test dataframes from data.'''\n",
    "        n = len(data)\n",
    "        test_size = int(test_split * n)\n",
    "        val_size = int(val_split * n)\n",
    "        test_data = data.iloc[list(range(0, test_size))]\n",
    "        val_data = data.iloc[list(range(test_size, test_size + val_size))]\n",
    "        train_data = data.iloc[list(range(test_size + val_size, n))]\n",
    "        return train_data, val_data, test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e4fd84",
   "metadata": {},
   "source": [
    "## Treinando o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd4a9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 0\n",
    "data_shuffled = Regularization_Linear_Regression_Model.shuffle_data(data_scaled, random_state = random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4483b0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data, test_data = Regularization_Linear_Regression_Model.train_val_test_split(data_shuffled, test_split = 0.2, val_split = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc4af98",
   "metadata": {},
   "source": [
    "### Vamos variar valores de lambda de 0 a 1 para verificar o que acontece com o custo e com a complexidade do modelo, mantendo os outros parâmetros constantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49185bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_reg_vals = np.arange(0, 1.1, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537c5e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_reg_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c618b6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = list(np.zeros(len(features)))\n",
    "b = 0\n",
    "alpha = 0.1\n",
    "models = []\n",
    "for lambda_reg in lambda_reg_vals:\n",
    "    model = Regularization_Linear_Regression_Model(train_data = train_data, features_name = features,\n",
    "                                        label_name = label, ws = ws, b = b, alpha = alpha, lambda_reg = lambda_reg,\n",
    "                                        random_state = random_state)\n",
    "    model.sgd(iterations = 100, batch_size = 10, print_loss = False)\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e2689f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(lambda_reg_vals)\n",
    "i_vals = list(range(n))\n",
    "lambda_row = {f'Model {i}': model.lambda_reg for (i, model) in zip(i_vals, models)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9380ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_models = pd.DataFrame([lambda_row])\n",
    "training_loss_row = {f'Model {i}': model.get_loss(train_data) for (i, model) in zip(i_vals, models)}\n",
    "val_loss_row = {f'Model {i}': model.get_loss(val_data) for (i, model) in zip(i_vals, models)}\n",
    "test_loss_row = {f'Model {i}': model.get_loss(test_data) for (i, model) in zip(i_vals, models)}\n",
    "complexity_row = {f'Model {i}': model.get_complexity() for (i, model) in zip(i_vals, models)}\n",
    "\n",
    "df_models = df_models.append(training_loss_row, ignore_index = True)\n",
    "df_models = df_models.append(val_loss_row, ignore_index = True)\n",
    "df_models = df_models.append(test_loss_row, ignore_index = True)\n",
    "df_models = df_models.append(complexity_row, ignore_index = True)\n",
    "df_models = df_models.rename(index ={0: 'Lambda', 1: 'Training Loss', 2: 'Validation Loss', 3: 'Test Loss', 4: 'Complexity'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c0df44",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32833c1",
   "metadata": {},
   "source": [
    "Verificamos justamente o esperado:\n",
    "\n",
    "- __Aumento do valor de Lambda $\\rightarrow$ Diminuição da Complexidade e Aumento do Custo do Modelo em todos os dados.__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63af8a6f",
   "metadata": {},
   "source": [
    "### Podemos verificar esse fato graficamente também:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc78ea15",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(lambda_row.values(), complexity_row.values())\n",
    "plt.plot(lambda_row.values(), training_loss_row.values())\n",
    "plt.plot(lambda_row.values(), validation_loss_row.values())\n",
    "plt.plot(lambda_row.values(), test_loss_row.values())\n",
    "plt.title('Complexity x Lambda')\n",
    "plt.xlabel('Lambda')\n",
    "plt.ylabel('Complexity')\n",
    "plt.legend(['Complexity', 'Training Loss', 'Validation Loss', 'Test Loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112eecd9",
   "metadata": {},
   "source": [
    "### Vamos adicionar também o cálculo do risco estrutural para cada modelo em cada conjunto de dados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2f50f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_sr_row = {f'Model {i}': model.get_structural_risk(train_data) for (i, model) in zip(i_vals, models)}\n",
    "val_sr_row = {f'Model {i}': model.get_structural_risk(val_data) for (i, model) in zip(i_vals, models)}\n",
    "test_sr_row = {f'Model {i}': model.get_structural_risk(test_data) for (i, model) in zip(i_vals, models)}\n",
    "df_models = df_models.append(training_sr_row, ignore_index = True)\n",
    "df_models = df_models.append(val_sr_row, ignore_index = True)\n",
    "df_models = df_models.append(test_sr_row, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df56cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_models = df_models.rename(index ={0: 'Lambda', 1: 'Training Loss', 2: 'Validation Loss', 3: 'Test Loss', 4: 'Complexity',\n",
    "                                    5: 'Training Structural Risk', 6: 'Validation Structural Risk', 7: 'Test Structural Risk'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a5c902",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac0545c",
   "metadata": {},
   "source": [
    "- A análise dos riscos estruturais dos modelos nos indica a mesma resposta da análise dos valores de perda do modelo nos dados de treinamento, validação e de teste, que nos leva a crer que o modelo com $lambda = 0$ é o melhor, dado que possui menores perdas e tais valores nos três datasets são muito próximos, negando a possibilidade de overfitting nos dados de treino. Tal modelo também é aquele com os menores riscos estruturais."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca409af",
   "metadata": {},
   "source": [
    "### Verificando que de fato os pesos das features diminuem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cfbba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for model in models:\n",
    "    print(f'Model {i}')\n",
    "    model.print_parameters()\n",
    "    print('\\n')\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a52db40",
   "metadata": {},
   "source": [
    "### Verificando isso graficamente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4272c00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ws_str = [f'w{i + 1}' for i in range(len(features))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f352eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ws_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163c594e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = []\n",
    "for w_str in ws_str:\n",
    "    w_list = []\n",
    "    for model in models:\n",
    "        w_list.append(model.get_parameters()[w_str])\n",
    "    ws.append(w_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f80c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "for i in range(0, len(features)):\n",
    "    plt.plot(lambda_reg_vals, ws[i])\n",
    "plt.title('Weights x Lambda')\n",
    "plt.xlabel('Lambda')\n",
    "plt.ylabel('Weight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec9b527",
   "metadata": {},
   "source": [
    "- Vemos que quando aumentamos suficientemente o valor de lambda, os pesos tendem a diminuir e a ficarem próximos de zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d98115e",
   "metadata": {},
   "source": [
    "- Poderíamos analisar também a distribuição dos pesos em cada modelo, verificando que se aproxima de uma distribuição normal na medida em que aumentamos o valor de $\\lambda$. No entanto, como há poucas features, a visualização de tal processo não é boa."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
